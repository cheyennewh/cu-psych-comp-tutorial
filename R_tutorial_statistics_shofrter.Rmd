---
title: "R_TUTORIAL_DESCRIPTIVES"
output: 
  html_document: 
    toc: true
    toc_float: true
--- 

# Welcome to the tutorial on statistics in R! 
Goals: 
1. Learn how to run simple statistical tests in R 
2. Learn how to visualize  the results of the models


```{r, include = F}
# packages needed for this tutorial
library(car)
library(effects)
```

#1. Correlations
```{r}
# as simple as it gets.... 
cor.test(mtcars$mpg, mtcars$wt)

```

# 2. T-tests
```{r}

# first let's check that our variance is homogenous using the Levene's test of Equality of Variances
leveneTest(mtcars$mpg, mtcars$vs) # we do not reject the null

# t.test= y, x (categorical)
t.test(mtcars$mpg, mtcars$vs)
```

#3. Anova 
Caution!!! there are many ways to run an anova and post-hoc tests in R. 
this is ONE way. please think about your data and model accordingly.
```{r}
# first we have to convert these binary variables into factors, instead of "numeric"
class(mtcars$vs)
class(mtcars$am)
mtcars$vs.factor <- as.factor(mtcars$vs)
mtcars$am.factor <- as.factor(mtcars$am)
class(mtcars$am.factor)
class(mtcars$vs.factor)

# one -way anova 
mtcars$am.factor <- as.factor(mtcars$am)
aov_1 <- aov(mpg ~ am.factor, data = mtcars )
summary(aov_1) # anova output
coefficients(aov_1) # coefficients (betas) 

# two -way anova 
aov_2 <- aov(mpg ~ am.factor*vs.factor, data = mtcars )
summary(aov_2)
coefficients(aov_2)
plot(allEffects(aov_2))
# post-hoc comparisons for 2x2 anova 
post_hoc <- TukeyHSD(aov_2)

# now let's take a look at the 6 contrasts of the 2x2 interaction: 
post_hoc$`am.factor:vs.factor`
```
#4. linear regression 
```{r}

#lm( y ~ x, data)
model1 <- lm(mpg ~ wt, data = mtcars )

# summary provides the useful output that you actually want to see: 
summary(model1)

# the effect package provides a simple way to visualize the results of your model
# here, we can quicky plot the linear regression slope with a 95% confidence interval
plot(effect("wt", model1))

# we can also make this plot prettier using similar base R arguments:
plot(effect("wt", model1), main = "", xlab = "miles per gallon", ylab = "weight of the car", rug = F)

```

# 5. Multiple regression using lm ()
```{r}
#Let's estimate the effect of weight on mpg, controlling for displacement and transmission.

# first, let's change our transmission variable into a factor. 
mtcars$am.factor <- as.factor(mtcars$am)

#lm( y ~ x + m + z, data)
model2 <- lm(mpg ~ wt + disp + am.factor , data = mtcars )
summary(model2) # so we see that weight still predicts mpg, even when controlling for gear and displacement

# you can also plot all of the model coefficients at once. The am.factor plot shows the group effect, with 95% CI.
plot(allEffects(model2))
```


# 8. interactions with continuous x continous variables 
```{R}
# let's visualize whether 2 continuous variables might interact 
coplot(mpg ~ wt | disp, data = mtcars,number = 3,
       col = "black", bg = "blue", pch = 21) 

# let's use lm to do the interaction (not aov)
model4 <- lm(mpg ~ wt* disp, data = mtcars )
summary(model4) 

# the effect plot automatically choses specific levels of the variables to display your interactions: 
plot(effect("wt:disp", model4), rug = F, main = "", ylab= "miles per gallon", xlab = "weight")

# you can also just plot the lines (without CI) this way: 
plot(effect("wt:disp", model4), multiline=T, rug = F, main = "", ylab= "miles per gallon", xlab = "weight")


```

# 9. more on model visualizations:

what about viewing the actual fitted data points (instead of the regression lines) ?
Added Variable plots (or partial regression plot) allow us to look at the fitted data of our variable of interest,
controlling for all of the other stuff in your model. 
```{r}

# look at partial regression plot (Added Variable Plot), with your X variable of interest.
avPlot(model4, "wt")

# make it prettier!
avPlot(model4, "wt", col = "Black", col.lines = "Blue", pch = 18, lwd= 3, 
       ylab = "miles per gallon",
       xlab ="weight", grid = FALSE,
       main = "Partial Regression Plot: Miles per gallon by Weight")


```

if you want to really make beautiful graphs and customizations of your models, 
we encourage you to experiment with ggplot!

This was only a brief introduction to the statistical modeling world of R. There are many more options available: mixed-effects modeling, mediations, structural equation models, bayesian estimation.

# Data Challenge:
Now it's your turn to check out some demographic information. Here is a pretend dataset from a behavioral task. 
This is just one subject's data, across many variables. 

```{r}

#Perform a t-test to determine whether there are group differences (PI, comP) 

# perform a linear regression on two variables of interest, and plot them

#  design your own analysis question and use the appropriate test of the options we covered today.

# optional: use ggplot to plot the raw data & the fitted results of your model.





```
