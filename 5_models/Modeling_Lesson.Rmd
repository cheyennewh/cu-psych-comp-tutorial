---
title: "R Tutorial: Modeling"
output: 
  html_document: 
    toc: true
    toc_float: true
--- 

 Welcome to the lesson on modeling in R! 
Goals: 
1. Learn about functions and syntax for models in R 
2. Practice examples of different model types 
3. Understand how to obtain model results in R  
4. Learn how to visualize the results of the models

 Let's get some data. 
We will use the same data we are already familiar with from the Descriptives lesson. We will compute a group for high and low ADHD, so we can use that in our models. 

```{r}
df <- read.csv("uncapher_2016_repeated_measures_dataset.csv")
head(df)
```

clean up the data for these models
```{r}
library(tidyverse)
 df <- df %>%   
    # only keep data from the task when distractor was present
   dplyr::filter(numDist == 6) %>%
   # only include high or low groups for multitasking.
   dplyr::filter(groupStatus != "") %>%
   # if adhd score is lower than mean, label "low", else label "high""
   mutate(adhdF = as.factor(ifelse(adhd < mean(adhd), "Low ADHD", "High ADHD")))
  
names(df)
```

# Basics for modeling in R 

## Functions:
### Basic models 
For each model type, there is a different function. Many of these are available in base R.

TTEST: `t.test()`
CORRELATION: `cor.test()`
LINEAR REGRESSION: `lm()`
ANOVA: `aov()`

Each function has their own set of arguments, based on the options available. 
Always use the `?` to see more details for the model you're using.

The first argument is always the model formula, which follows a standard syntax.

## Syntax: 
### Basic model formula
For any type of model with an IV and DV, there is a common syntax. The "~" delineates the direction of the model, where the DV is always on the left of the ~ and the predictors are always on the right.

Y ~ X

Example of t-test
```{r}
t.test(dprime ~ adhdF, data = df)
```

*When working with a dataframe, you can either index your data with `$` or provide a second argument in the function, delineating which data your variables come from -- the output will be identical. However, we recommend the 2nd option, as it ensures that you will be always working with columns from the same data frame, and is less susceptible to mistakes*

Example of a linear regression
```{r}
lm(dprime ~ adhd, data = df)
```

Example of an anova
```{r}
aov(dprime ~ adhdF, data = df)
```

### To add multiple factors 
(in anova or linear regression), simply add  a "+" for each variable on the right of the formula 

*Y ~ X1 + X2 + X3*

Here is a linear model example with 2 predictors (continuous)
```{r}
lm(dprime ~ adhd + bis, data = df)
```

Here is an ANOVA example with 2 predictors (continuous)
```{r}
aov(dprime ~ adhdF + bis, data = df)
```

### Interactions 
When you want to test the interaction between two variables, add a ":". 
The colon will automatically create an interaction regressor between x1 and x2.
You still need to add each variable alone to calculate their main effects. 

Y ~ X1 + X2 + X1:X2

Alternatively, a shortcut is to use an asterix "*" that provides a shortcut. It will automatically create an interaction regressor AND the main effects for each variable.

 Y ~ X1*X2  is identical to Y ~ X1 + X2 + X1:X2

Example of an interaction between 2 continous variables in linear regression:
```{r}
lm(dprime ~ adhd*bis, data = df)
```

Example of an interaction between 2 categorical variables in anova
```{r}
aov(dprime ~ adhdF*groupStatus, data = df)
```

In the case where there is no IV/DV (i.e. correlation between 2 continuous variables), there is no "~".
Instead there is a ","" to delineate 2 different variables.

Example of a correlation
```{r}
cor.test(df$dprime, df$adhd)
```


## Output of models 
The output of your code will depend on the model type. Linear models have much more information to output relative to t-tests, so they require an additional "summary()" call on the model to see the full results. 

### Linear models
the output includes the following:
a) the formula of your model 
b) residuals
c) coefficients of the model (estimate, st. error, t-value, p-value)
d) overall model results (RSE, R-squared, F-statistics, p-value)

```{r}
# first, we will save our model object and call it lm1
lm1 <-  lm(dprime ~ adhd, data = df)
# then we will ask for the summary of the model results
summary(lm1)
```

We can index specific components of the output. What if I want to save the coefficients to a table?
```{r}
summary(lm1)$coefficients

# save the table for later
my_table <- summary(lm1)$coefficients
save(my_table, file = "my_table.Rda")
```

### Anova
The output includes the results for each factor in the model.

Here is an example for a one-way anova
```{r}

a1 <- aov(dprime ~ adhdF, data = df)

summary(a1)
```

Here is an example for a 2x2 anova
```{r}

a2 <- aov(dprime ~ adhdF * groupStatus, data = df)

summary(a2)
```
## Interpreting Regression Models

Although we've presented several types of models here, all of them are, at their root, cases of regression! Although regression coefficients are sometimes difficult to interpret, we think this is crucial! Below, we go through several examples:

*Note, we'll also load the arm package, which helps to display model outputs in a handy way*

### Regression with a categorical (binary) predictor

Let's estimate dprime as a function of adhdF group ('High ADHD' or 'Low ADHD') here
```{r}
suppressWarnings(suppressMessages(library(arm)))
categoricalRegMod = lm(dprime ~ adhdF, data = df)
display(categoricalRegMod)
```

Okay, so let's break down what this means.

  * First, we see our model equation at the top of the output. Handy!
  * **(Intercept)**: On the next line, we have estimates for the intercept of this model, and an estimate of the standard error for this intercept. Crucially, the intercept here in the regression model is the *estimated value of the outcome variable when all predictors are at their default.* Ah, but what's this 'default'? This is something we have to be very careful with when setting up our models.
    * For numerical predictors the default is 0
    * For character variables (strings) the default is the first alphabetical possibility
    * For factor variables the default is the first level of the factor (level 1). 
    * So here, average dprime is estimated to be 1.13 with standard error 0.09 for 'High ADHD' subjects
    * Note that the intercept here is exactly the same as the estimate for the 'High ADHD' group in the two-sample t-test earlier
  * **adhdFlow ADHD**: On the third line, we have the estimate and estimated standard error for the beta coefficient for adhdF -- our binary variable that takes values 'Low ADHD' and 'High ADHD'. We can interpret the estimate such that on average, an individual in the 'Low ADHD' group is estimated to have a dprime score -0.11 units different (with a standard error of .13) units from an individual in the 'High ADHD' group. 
    * Importantly, it is pretty clear that we don't have a very accurate estimate for our adhdF predictor here! If the standard error is 0.13 dprime units when the estimated value of the predictors is only -0.11 dprime units, our uncertainty in the estimated average difference between the two groups is huge compared to the estimated difference itself.


### Regression with a continuous predictor 

Let's estimate dprime as a function of a continuous variable, the number of hits (hitCount). This variable ranges from 1-20

```{r}
continuousRegMod = lm(dprime ~ hitCount, data = df)
display(continuousRegMod)
```

Okay, let's interpret.

  * **Intercept**: The intercept is tricky here. This estimate is for the average subject who has a value of 0 for hitCount. This isn't possible!
    * This is definitely something to watch out for! One solution to this problem (so our intercept will be more meaningful) is to mean-center our continuous predictor, so that the intercept will represent the estimated dprime for a subject with an *average* hitCount score.
  * **hitCount**: The hitCount predictor here indicates that on average, an individual with 1 more hit than another individual is estimated to have a dprime score 0.07 higher, with a standard error of .01. 
    * Compared to the estimate of 0.07, the standard error of the hitCount estimate 0.01 is pretty small, so we have relatively low uncertainty in our estimate of the hitCount predictor. (This makes a lot of sense, because the hitCount is probably being used to calculate dprime here)

### Regression with a mean-centered continuous predictor

Mean-centering is often a good idea for interpreting our models. To make the intercept easier to interpret, let's try mean-centering hitCount here.

```{r}
# check what the mean for hitCount is and mean-center hitCount
mean(df$hitCount)
df = mutate(df,
            hitCountMeanCenter = hitCount - mean(hitCount))


continuousRegMeanCenterMod = lm(dprime ~ hitCountMeanCenter, data = df)
display(continuousRegMeanCenterMod)
```

While mean-centering might feel confusing at first, this model is actually much easier to interpret in many ways.

Importantly:

  * All the estimates for the hitCount predictor are unchanged by mean centering! In a linear regression with one predictor, this will always be true (but we have to be careful about this if there are interaction terms)
  * The intercept now represents the estimated dprime for an average subject at the average hitCount, `r mean(df$hitCount)`. This is much more helpful than the estimated dprime for a subject with 0 hits! Notice that the standard error for this estimate has also gone down from the previous model -- this is because it is easier to accurately estimate an intercept at the mean level of a predictor than at a value for which no data exist!

## Visualizing models 

There are many ways (and packages) to plot the output of your models. 
We will start with a handy package, [`effects`](https://cran.r-project.org/web/packages/effects/effects.pdf), that plots the effects of many model types.

```{r}
#install.packages("effects")
library(effects)
```

Let's plot the effect of ADHD on dprime from our linear model.
```{r}
# This function requires the X variable in quotes, and the saved model object
plot(effect("adhd", lm1))

```


There are many more options you can add to customize the plot! 
```{r}

plot(effect("adhd", lm1), rug = F, colors = "red", xlab = "ADHD symptoms", ylab = "D-prime",
     axes=list(grid=TRUE))

```



Visualizing continuous x continous interactions 
```{r}
plot(effect("adhdF:groupStatus", a2), xlab = "", colors = c("Blue", "Black"))
```

if you want to really make beautiful graphs and customizations of your models, 
we encourage you to experiment with ggplot!

## Advanced models 

For more advanced modeling, you may need to download packages for those model types. Two common examples you may see in graduate level statistics include:
`lmer()` for mixed effects modeling using the `lme4` package 
`mediation()` for mediations using the `mediate` package
However, the opportunities for modeling in R are endless! There is a package for any kind of model you can think of.

