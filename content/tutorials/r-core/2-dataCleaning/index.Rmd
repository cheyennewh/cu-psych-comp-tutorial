---
title: "Getting your data ready for analysis"
author: "Ellen Tedeschi"
subtitle: "CU Psych Scientific Computing Workshop"
tags: ["core", "r"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(here)
```

## Goals of this lesson

Students will learn:

1. How to open various data types in R
2. How to check for missing or problematic data and address issues.
3. How to filter, rearrange and shape data in preparation for analysis. 

## Install Packages

`tidyverse` is one package with many tools.  We will introduce you to this package in this lesson, and continue using `tidyverse` in the next module.

## Getting your Data into R

### Entering data directly

In some cases, you may want to enter data directly into R. This is easy with a small number of cases.

```{r}
# Direct Data Entry
dataframe <- tibble(name = c("Monica", "Michelle", "Paul", "Ellen"),
                    score = c(20, 16, 35, 19),
                    year = c(2, 5, 2, 1))
```


### Reading data into R

It's also easy to introduce errors this way, and with a lot of data it would get tedious. Most of the time, you'll be reading data from an external file (.txt or .csv), or opening up an existing dataset in R. Once you find the location of your files, what you do next will depend on the file format.

#### What's your working directory and where is the file you want?

Your working directory is the directory (and subdirectories) where the files you're working with are stored. Whenever possible, you should open the files you run in R from a .Rproj link; by doing so, the folder in which that file was located will automatically be your working directory.

You can check your working directory using the `here()` function.

```{r}
here()
```

You can access a list of the files within your working directory using the `list.files()` function...

```{r}
list.files(here())
```

...as well as in the "Files" tab in the module at the bottom right corner of your RStudio window.

#### What kind of file do you have?
```{r}
# For .csv files
mydata <- read_csv(here("content", "tutorials", "r-core", "2-dataCleaning", "Study1.csv"))


# Other options include:
# read_tsv, for tab-delimited files
# read_delim, for when you have a separator that's not standard

## For getting data that's already in R-format
load(here("content", "tutorials", "r-core", "2-dataCleaning", "Study1.rda"))

## For getting data from another program:
#install.packages("foreign")
#library(foreign)

#read.spss("<Path to file>")
#read.dta("<Path to file>")
```

Remember, all of these commands can have arguments that will help R make sense of your data. To find out what arguments are possible, put a question mark in front of the name of the function (e.g. `?read.table`). For options not listed here, Google "<your data type> into R" and you should find instructions.

## Inspecting your data

Now you have data, time to get some results! But wait! Are you sure this data is ok? Doing some basic steps to inspect your data now can save you lots of headaches later, and R makes it really easy.

Start by checking that you have the expected number of rows and columns in your data frame. You can do this by looking at the Environment window, or by asking R.

### How is your data frame structured?
```{r}
# Number of rows and number of columns
numberSubjects <- nrow(mydata)
ncol(mydata)

# Names of columns
names(mydata)

# look at the first few rows
mydata
```

### Brief intro to `tidyverse`

`tidyverse` is not one package, but instead it's multiple packages grouped together. They're all made by the same developers, so the functions play nicely together. If you've done any R before, there are some you're probably familiar with, like `ggplot` and `dplyr`, and others that are more obscure. These are all very useful packages, but they do have some `tidyverse`-specific syntax that we'll be using. (Note: the following paragraphs are an almost verbatim verision of what's in our standalone `tidyverse` tutorial, written by Monica, which can be found on the Github page).

Enter the pipe `%>%`! The pipe does one simple, but key, thing: **takes the object on the left-hand side and feeds it into the first argument of the function on the right-hand side.** This means that:

* `a %>% foo()` is equivalent to `foo(a)`. Fine and good.
* `a %>% foo() %>% bar(arg = TRUE)` is equivalent to `bar(foo(a), arg = TRUE)`. Now, nested function calls read left-to-right!
* Most common use case: `df_new <- df_old %>% foo() %>% bar(arg = TRUE) %>% baz()` is equivalent to `df_new <- baz(bar(foo(df_old), arg = TRUE))`. Now, you can chain a series of preprocessing commands to operate on a dataframe all at once, and easily read those commands as typed in your script. No more accidentally not running some key preprocessing command that causes later code to break!

* take as their first argument the object to be operated upon
* return the same object (or an analog of said), but now operated upon

Essentially all functions from the tidyverse are pipe-safe, but bear this in mind when trying to incorporate functions from base R or other packages into your tidy new world.

### Rename a variable

We'll demonstrate the `%>%` command for our first cleaning step. Look back at your data frame.  What is that fifth variable? What does that even mean? Luckily, this is your study and you know that it's a personality questionnaire measuring neuroticism. Let's fix that using `recode()` from the `dplyr` package.

```{r}
# Simple command with no pipe, commented out so we don't run both.
#mydata <- rename(mydata, Neuroticism=Personality) ## NewVariableName = OldVariableName

# Simple command with pipe
mydata <- mydata %>% 
  rename(Neuroticism=Personality) 

mydata
```

It might seem like the first version is simpler, but it won't be once you start stacking functions together. For example, let's rearrange our variables (using `select()`) while also renameing the variables called T1, T2, T3, and T4. Note that we keep adding `%>%` between each function.

```{r}
mydata <- mydata %>% 
  rename(Day1 = T1, Day2 = T2, Day3 = T3, Day4 = T4) %>% 
  select(ID, Condition, Age, everything())
  
# using everything() within select() means you don't have to type all the variables out.
```

### Check for missing data

One problem you may have is missing data. Sometimes this is something you already know about, but you should check your data frame anyway to make sure nothing got missed in a data entry error. For small datasets, you can do this visually, but for larger ones you can ask R.

```{r}
# This command asks for rows that are not complete. 
mydata %>% 
  filter(!complete.cases(.))
```

In this case, the missing value is the Age value in row 39. You know you have this info somewhere on a paper form, so you go dig it up and want to replace it.

```{r}
# Directly replace a missing value
mydata <- mydata %>% 
  mutate(Age = if_else(ID == 39, 30L, Age))
```

### Check for correct values

To look at some of the other variables, let's use the `table()` function. This works well for factors or variables with only a few different values. Our condition and sex variables are good here.
```{r}
# Make tables of categorical variables
table(mydata$Condition)

table(mydata$Sex)
```

When we look at the table for the Sex variable, we see another data entry problem. We have a third category that should really be another case of "Female". Here we are going to use the function `recode` from the `dplyr` package. 

```{r}
# Replace bad category label
mydata <- mydata %>% 
  mutate(Sex = dplyr::recode(Sex, Femle = "Female"))

table(mydata$Sex)

table(mydata$Age)
```

Now let's look at the continuous variables. You can also look at these with the table function, but sometimes it's easier to visualize. The `hist()` function, which creates histograms, is good here.

```{r}
# Create histograms to see data distribution
hist(mydata$Age)
hist(mydata$Neuroticism)
```

Looks like we have a potential outlier on the neuroticism score. This could be an entry error, but it could also be a real value that just happens to be really low. This is why data inspection is so important for later analysis -- now you know that value is there, it's up to you to decide how to deal with it.

### Filtering data 
Let's say we have decided a prori to exclude outliers 3SD above or below the mean. We will use the filter() function in `tidyverse` to clean the dataset.

```{r}
# mark upper and lower boundaries of data
# anything above upper or below lower will be excluded as outlier.
upper <- mean(mydata$Neuroticism) + 3*sd(mydata$Neuroticism)
lower <- mean(mydata$Neuroticism)- 3*sd(mydata$Neuroticism)

mydata <- mydata %>% 
  # only keep values within this range! 
  filter( Neuroticism > lower & Neuroticism < upper)

# check that we excluded 1 outlier. 
nrow(mydata)
hist(mydata$Neuroticism)
```

## Getting ready for analysis

Now that we've gone through and cleaned up the problems, you can think ahead to how you'll want to use this data. 

### Recoding variables

Sometimes we want to treat categorical variables as factors, but sometimes we want to pretend they're numeric (as in a regression, when binary variables can be coded as 0 and 1). Right now, Condition is coded as a binary numeric variable, but that's not very informative, so you'd rather have the values be descriptive. Here, the function `recode()` from the `dplyr` package is really useful again, but we're going to use a different version that will also turn the variable from numeric or factor data. We'll also create a second variable instead of overwriting Condition.

```{r}
# Transform into factor with labels using mutate () and recode_factor() 
mydata <- mydata %>%
  mutate(ConditionF = recode_factor(Condition, '0'='Control', '1'='Treatment'))

# check that your variable is now recoded as you'd like! 
summary(mydata$ConditionF)
```

### Calculating new variables

You may also want to recalculate or rescale some variables. For example, we can turn Neuroticism into a Z score, or calculate an average response across the four time points. Here we are using `mutate()`, a dplyr function used for adding new variables to a data frame. Mutate is nice because it makes use of other functions, such as `scale()` to make Z scores or `rowMeans()` to calculate means.

```{r}
# calculate Z scores using mutate()  and scale() 
mydata <- mydata %>%
  mutate(NeuroticismZ = scale(Neuroticism, center = TRUE, scale = TRUE),
         # Scale creates a matrix with width 1,
         # sometimes this messes things up so we'll convert it to vector        
         NeuroticismZ = c(NeuroticismZ))

hist(mydata$NeuroticismZ)

# calculate means across multiple columns using rowMeans
# note that cbind is necessary here because rowMeans expects a Matrix

mydata <- mydata %>% 
  mutate(DayMean = rowMeans(cbind(Day1, Day2, Day3, Day4)))
```

### Combining data from multiple sources

Sometimes, data might be spread across multiple spreadsheets, and you'll want to combine those for your analysis. For example, maybe this study had a follow-up survey on Day 30. Scores from that survey were entered into another spreadsheet, which only has the subject ID and that score. We want to include that score into our data.

Here I'm using the function `merge()`, which is one of the base functions in R, not part of the tidyverse.

```{r}
#First load the new data
mydata2 <- read_csv(here("content", "tutorials", "r-core", "2-dataCleaning", "Study1_Followup.csv"))

# Merge the two dataframes. To make sure the rows match up, we use the 'by' argument to specify that IDs should match. That way even if the data is in a different order you will get scores matched togther correctly.

mydata <- inner_join(mydata, mydata2, by = "ID")

# Note that mydata has one less row than mydata2 because we removed that outlier. By default, merge will use only cases of the 'by' variable that exist in both dataframes, so it drops that one row from mydata2 when merging. You can use additional arguments to change this if you want to keep all observations from one or both frames, even if it means blank cells.
```

There are joining functions in the tidyverse that you can use if you'd like; feel free to ask about these and how they're different from `merge()`.

### Shaping data

Finally, you may want to change the layout of your data. Right now, our data frame is in "wide" format, which means that each row is a subject, and each observation gets its own column. For some analyses, you'll need to use "long" format, where each row is an observation, and columns specify things like time and ID to differentiate the observations. There are lots of packages that can handle data reshaping, but I'll show the `pivot_longer()` and `pivot_wider()` functions from `tidyr`.

```{r}
# names_to is the name for the new column that will identify which observation it is
# values_to is the name for the new column that will have the actual values in it.
# In theory, you can name these whatever you want,
# but to keep everyone on the same page, name the key "Time" and the value "Score".

mydata_Long <- pivot_longer(mydata, cols = starts_with("Day"), names_to="Time", values_to="Score")


# pivot_wider lets you go back the other direction. This should be identical to the original mydata 

mydata_Wide <- pivot_wider(mydata_Long,
                           id_cols = c(ID, Age, Sex, Condition, Neuroticism),
                           names_from = Time, values_from = Score)
```

## Saving your work

Once you've created a data cleaning script like this one, you'll have a record of all the edits you've made on the raw data, and you can recreate your cleaned data just by running the script again. However, it's often easier to save your cleaned data as its own file **(never overwrite the raw data)**, so when you come back to do analysis you don't have to bother with all the cleaning steps. 

You can always save data frames as a .csv for easy sharing and viewing outside of R.

```{r, eval = FALSE}
# Write data to a .csv
write_csv(mydata, path = here("content", "tutorials", "r-core", "2-dataCleaning", "Study1_Clean.csv"))
```

However, you can also save in an R format that lets you save multiple variables/objects in the same file. For example, you might want to have a long and wide format, or one dataframe with all the data and one with just subject information. Saving as a .rda file allows you to save multiple objects at once for easy loading into R. You can also have the outputs of statistical models saved in these, along with their data.

```{r, eval = FALSE}
# Save a .rda. Add as many objects as you want, separated by commas
save(mydata_Long, mydata_Wide, file = here("content", "tutorials", "r-core", "2-dataCleaning",  "Study1_clean.rda"))
```

## Data Challenge!

Go to the file [CleaningChallenge_NoCode.Rmd](2-dataCleaning/CleaningChallenge_NoCode.Rmd) for your data challenge. This challenge will include skills from this tutorial, but it will also ask you to explore new functions we didn't cover. Remember to use the help command in R (?<function name>) and google as necessary. You'll also be using three files from "Study 2", so make sure they're in your folder: [README_Study2.txt](2-dataCleaning/README_Study2.txt), [Study2_Subjects.csv](2-dataCleaning/Study2_Subjects.csv), and [Study2_Trials.csv](2-dataCleaning/Study2_Trials.csv).

## Future Directions

Congratulations! You've now cleaned some data in R and you're ready to start working with your data. This tutorial only went over some basic cleaning steps. As you work with your own data, you may find yourself needing other tools. 
