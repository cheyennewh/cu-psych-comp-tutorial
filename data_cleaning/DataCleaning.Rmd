---
title: "Data Cleaning Draft"
author: "Ellen Tedeschi"
date: "October 21, 2017"
output: html_document
---


## Entering data directly

In some cases, you may want to enter data directly into R. This is easy with a small number of cases.

```{r}

# Direct Data Entry

score <- c(20, 16, 35, 19)

name <- c("Monica", "Michelle", "Paul", "Ellen")

year <- c(1, 1, 4, 3)

# MT: do we need to clarify that not specifying colnames when initializing using data.frame() will make the colnames be the varnames from the global env? I think I want to teach people to explicitly name their args whenever possible, though sometimes it's obvious enough that you don't have to
dataframe <- data.frame(name, score, year)

```


## Reading data into R

It's also easy to introduce errors this way, and with a lot of data it would get tedious. Most of the time, you'll be reading data from an external file, or opening up an existing dataset in R. Once you find the location of your files, what you do next will depend on the file format.

```{r}

# What's your working directory and where is the file you want?

getwd()

list.files()

#setwd(#Set you working directory) 

# What kind of file do you have?

## For .txt, .csv and tab-deliminated files

?read.table
 
mydata <- read.table("Study1.csv")
# well that didn't work...

mydata <- read.table("Study1.csv", header=TRUE, sep=",")
# better!

# Other options include:
# read.csv, just like read.table but with better defaults for .csv
# read.delim, for when you have a separator that's not standard

## For getting data that's already in R-format

load("Study1.rda")

## For getting data from another program:
install.packages("foreign")
library(foreign)

read.spss("<Path to file>")
read.dta("<Path to file>")
# remember, all these may take more arguments to get your data exactly the way you want
# Use the help functions to get more details

```

## Inspecting your data

Now you have data, time to get some results! But wait! Are you sure this data is ok? Doing some basic steps to inspect your data now can save you lots of headaches later, and R makes it really easy.

Start by checking that you have the correct number of rows and columns in your data frame. You can do this by looking at the Environment window, or by asking R.

```{r}
# By "correct" do you mean "expected"?

# Number of rows and number of columns
nrow(mydata)
ncol(mydata)

# Names of columns
names(mydata)

# look at the first few rows
head(mydata, 10)

```

### Rename a variable

Ok, looks good. But what is that fifth variable? What does that even mean? Luckily, this is your study and you know that it's a personality questionnaire measuring neuroticism. Let's fix that.

```{r}
# These do the same thing, but the second one is more explanatory, and better for reproducibility
# MT: Maybe only show people how to do it the second way then?

names(mydata)[5] <- 'Neuroticism'
names(mydata)[names(mydata) == 'Personality'] <- 'Neuroticism'
names(mydata)

```

### Check for missing data

On problem you may have is missing data. Sometimes this is something you already know about, but you should check your data frame anyway to make sure nothing got missed in a data entry error.

```{r}

is.na(mydata)
# that's a lot to go through, just tell me if there are any
sum(is.na(mydata))
# that's better, now I know there's 1 missing value

```

In this case, the missing value is the Age value in row 39. You know you have this info somewhere on a paper form, so you go dig it up and want to replace it.

```{r}

# Again, these do the same thing, but the second one is more reliable and easier to interpret.
mydata[39, 2] <- 30

mydata$Age[mydata$ID==39] <- 30

```

### Check for correct values

To look at some of the other variables, let's use the table function. This works well for factors or variables with only a few different values. Our condition and sex variables are good here.
```{r}

table(mydata$Condition)
# looks good
table(mydata$Sex)

```
This looks like another data entry problem. We have a third category that should really be another case of "Female".We can fix all of these at once, but sometimes messing with factors is more complicated than numeric variables.

MT: In my experience it's best to ALWAYS read CSVs with read.csv(stringsAsFactors = FALSE). Since factor levels can be such a bitch, I find that it's safer to only render columns as factor AFTER you've cleaned problems with factor level naming in the character vector. and also, can get around another data cleaning issue where numeric cols render as factor because there's one random character element in there. so this might change a couple of things you do here?

```{r}

# MT: I'm fine with this, esp cause the mutate alternative involves using an ifelse() call. But should we use dplyr::mutate?
mydata$Sex[mydata$Sex=="Femle"] <- "Female"

table(mydata$Sex)

# MT: wouldn't have to use droplevels() (which is v useful when you do need it) if the data were loaded in with StringsAsFactors = FALSE and it wouldn't have been factor type to begin with
mydata$Sex <- droplevels(mydata$Sex)

```

Now let's look at the continuous variables. You can also look at these with the table function, but sometimes it's easier to visualize. The histogram function is good here.

```{r}

hist(mydata$Age)
hist(mydata$Neuroticism)

```

Looks like we have a potential outlier on the neuroticism score. This could be an entry error, but it could also be a real value that just happens to be really low. This is why data inspection is so important for later analysis - now you know that value is there, it's up to you to decide how to deal with it.

## Getting ready for analysis

Now that we've gone through and cleaned up the problems, you can think ahead to how you'll want to use this data. 

### Recoding variables

Sometimes we want to treat categorical variables as factors, but sometimes we want to pretend they're numeric (as in a regression, when binary variables can be coded as 0 and 1). Right now, Condition is coded as a binary numeric variable, but we'd like to use it as a factor, and it would be good if the values were actually labelled. However, rather than overwriting the variable, let's make a second one.

```{r}

# Transform into factor
mydata$ConditionF <- as.factor(mydata$Condition)

# Specify labels for the factor levels
# MT: Should we use plyr::mapvalues here? Doesn't require coercing to factor first! Also... just better?
# mapvalues(mydata$ConditionF, from = c(0, 1), to = c("Control", "Treatment"))
mydata$ConditionF <- factor(mydata$ConditionF, levels=c("0", "1"), labels=c("Control", "Treatment"))

```

### Calculating new variables

You may also want to recalculate or rescale some variables. For example, we can turn Neuroticism into a Z score, or calculate an average response across the four time points.

```{r}

# calculate Z scores
# MT: You can use the scale() function to do this. like: scale(mydata$Neuroticism, center = TRUE, scale = TRUE). center = TRUE mean-centers the data, and scale = TRUE scales by standard dev if center = TRUE and the data was mean-centered.

mydata$NeuroticismZ <- (mydata$Neuroticism - mean(mydata$Neuroticism)) / sd(mydata$Neuroticism)
hist(mydata$NeuroticismZ)

# one way to average multiple columns
# Food for thought: Why can't we just use the mean() function here? (added by MT)
mydata$Average <- rowMeans(mydata[c("T1", "T2", "T3", "T4")])
# another way
# MT: I think let's not show them this way because hard-coding the n to divide by is less reproducible
mydata$Average <- (mydata$T1 + mydata$T2 + mydata$T3 + mydata$T4)/4

```

### Shaping data

Finally, you may want to change the layout of your data. Right now, our data frame is in "wide" format, which means that each row is a subject, and each observation gets its own column. For some analyses, you'll need to use "long" format, where each row is an observation, and columns specify things like time and ID to differentiate the observations. There are lots of packages that can handle data reshaping, but I'll show the gather and spread functions from tidyr

```{r}

library(tidyr)
# Key is the name for the new column that will identify which observation it is, and value is the name for the new column that will have the actual values in it. You can name these whatever you want.
mydata_Long <- gather(mydata, key="Time", value="Score", T1:T4)

# Spread lets you go back the other direction. This should be identical to the original mydata 
mydata_Wide <- spread(mydata_Long, key="Time", value="Score")

```

## Saving your work

Once you've created a data cleaning script like this one, you'll have a record of all the edits you've made on the raw data, and you can recreate your cleaned data just by running the script again. However, it's often easier to save your cleaned data as its own file **(never overwrite the raw data)**, so when you come back to do analysis you don't have to bother with all the cleaning steps. 

You can always save data frames as a .csv for easy sharing and viewing outside of R.

```{r}

write.csv(mydata, file="Study1_Clean.csv")

```

However, you can also save in an R format that lets you save multiple variables/objects in the same file. For example, you might want to have a long and wide format, or one dataframe with all the data and one with just subject information. Saving as a .rda file allows you to save multiple objects at once for easy loading into R. You can also have the outputs of statistical models saved in these, along with their data.

```{r}

# Add as many objects as you want, separated by commas
save(mydata_Long, mydata_Wide, file="Study1_clean.rda")


```

# Your turn!

Now it's your turn to try opening and cleaning a new data set. There are issues with the raw data, so make sure to inspect the variables and make changes when needed. Below is a rough outline you can follow, in the end, you should have a file in the long format with XX rows and XX columns.

```{r}

# Read the data into R. The file is called "Study2_Subjects.csv". Check out the separate "README_Study2.txt" file for information on the study and variables.


# View the first 5 rows of the data frame


# Examine the data for issues. Some things to look for: are the column names logical? Are there missing values? Do the variable types make sense? (ie does everything that's a factor make sense as a factor, and does everything that's numeric make sense as numeric?) Are there values that don't make sense? Fix any issues you find.


# This study has three conditions, based on how long the subject kept their hand in cold water. Right now this is categorical, but you may want to look at it continuously. Make a new condition variable that's numeric.


# Each subject in this study filled out three related questionnaires, but these scores can be combined into a composite score but taking the sum. Create a new variable for this score.


# You see there's another .csv file for Study 2, this one's called Study2_Trials.csv. Open it and take a look. This frame has the same 10 subjects, but now it has reaction time data from a task with 30 trials. Create a new data frame that combines this with your subject level information. Hint: ?merge, by="ID"


# Now that you have a one frame with all the data, put it into long format. Each subject should have one row for each trial.


# You want to do a separate analysis only on the people who had a high score on the anxiety questionnaire. Create a new data frame with just the individuals who got 50 or higher. (Hint, check out ?subset for help)



# You should have 4 data frames now (subject level, trial level, combined, and the subset). Save them as a new .rda file for future use



```


## Future Directions

Congratulations! You've now cleaned some data in R and you're ready to start the real analysis. This tutorial only went over some basic cleaning steps, as you work with your own data, you may find yourself needing other tools. We used functions that come with the basic level of R, but there are many specialized packages that can be used for cleaning and manipulating data structures.

Put more here about dplyr, tidyverse, etc.