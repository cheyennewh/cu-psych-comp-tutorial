---
title: "Presentation Ninja"
subtitle: "âš”<br/>with xaringan"
author: "Monica Thieu & Paul Bloom"
institute: "Dept of Psychology, Columbia University"
date: "2021-03-10 (updated: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
    css: ["theme.css", "fonts.css"]
    lib_dir: libs
    nature:
      highlightLanguage: r
      highlightStyle: solarized-light
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
require(tidyverse)
require(magrittr)
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(echo = FALSE, dev = "svglite")

se <- function (x, ...) {return (sd(x, ...)/sqrt(length(x)))}
```

# Outline

--

First, we'll review error intervals based on _how they're calculated._

--

Then, we'll explore different techniques for visualizing those intervals.

---

class: inverse, middle

# Two different types of error intervals

---

.pull-left[

## Analytical intervals

- calculated _exactly_ with **formulas**
- computationally _fast_
- _wholly_ reliant on distributional assumptions

]

.pull-right[

## Numerical intervals

- calculated _approximately_ over many **iterations**
- computationally _slower_
- _less_ reliant on distributional assumptions (and can work in the absence of assumptions!)

]

--

In the majority of cases, errors should agree when estimated using either method. In general, we recommend taking more time to estimate numerical intervals _unless:_

- You are confident that distributional assumptions will hold
- It is temporally/computationally infeasible to estimate numerical intervals

TODO: note regularly that we like methods that make you look at your data/intermediate outputs

---

class: inverse, middle

# Analytical error intervals

--

- Ordinary least squares intervals

--

- Maximum likelihood intervals

---

## OLS intervals

This encompasses the parameter estimates and standard errors calculated from _ordinary least squares regressions._

--

Remember, OLS regressions capture a [variety](https://lindeloev.github.io/tests-as-linear/) of statistical scenarios (t-tests, correlations, ANOVAs, and more).

--

In general, the error intervals we can get from these tests are a **standard error of the parameter estimate** and its associated **confidence interval.**

--

These values are directly related to one another by the following general formula:

$$CI_p = \overline{T} \pm t_{crit} \times SE$$

--

In most cases, the distance from the test statistic to one CI bound is equal to the standard error times the critical value for the desired confidence level.

--

If we assume a 95% confidence interval, and that the sampling distribution of the test statistic is _t_-distributed or normally distributed, the CI is approximately equal to the test statistic $\pm$ 2 SEs.

---

Don't forget! Dr. Niall Bolger sez:

--

**Over many theoretical repeated runs of a study, N% of the N% confidence intervals for all runs of the study are expected to overlap with the true parameter.**

--

A single calculation of the N% confidence interval does _not_ reflect an N% probability that the true parameter lies within that interval.

---

## ML intervals

TODO: Incorporate photo of parameter landscape to hammer home that when you don't know the landscape you can't take the second derivative

--

Maximum likelihood models can fit multilevel data, or other data that cannot readily be modeled with ordinary least squares regression.

--

ML models, like those from `lme4`, produce standard errors by taking the second derivative of the parameter log-likelihood function with respect to the parameter of interest, $\theta$, at the value of the maximum-likelihood estimate of said parameter, $\hat{\theta}$.

--

When the log-likelihood function is estimated well, ML standard errors are a good approximation of the standard deviation of the sampling distribution for multilevel model coefficients.

--

However, if ML estimation _fails to converge_ on a set of parameter estimates, this may mean that the log-likelihood function _violates the assumptions needed_ to consider the SE a valid estimate.

--

See [Penn State STAT 504 Spring 2005 lecture slides](https://personal.psu.edu/abs12/stat504/Lecture/lec3_4up.pdf) for further reference.

---

class: inverse, middle

# Numerical intervals

--

- Bootstrapping

--

- Bayesian Monte Carlo sampling

---

## Bootstrapping

Randomly resample complete observations from the base dataset with replacement many times, calculating the test statistic once for every resample. Collate all estimates of the test statistic across bootstrap iterations into a sampling distribution.

TODO: say it's important to LOOK at the bootstrapping distribution

If the resulting sampling distribution is normal, the bootstrapped standard error is equal to the **standard deviation of the sampling distribution.**

If the resulting sampling distribution is asymmetrical (and not normal), the lower and upper bounds of bootstrapped standard error interval can be calculated by taking the **16% and 84% percentiles of the sampling distribution.** (If the sampling distribution is symmetrical but not normal, this is equivalent to taking the middle 68% of the distribution.)

---

### Statistics you can bootstrap:

A sampling distribution can be bootstrapped for _basically any_ test statistic, no matter how complex its calculation is. Thus, bootstrapping is especially useful in cases where the test statistic might not have a normal sampling distribution, and analytical intervals might be inappropriate.

TODO: List things you can bootstrap

---

## Bayesian Monte Carlo sampling

---

class: inverse, middle

# Visualization

---

class: center, middle

The purpose of any graph, error bars or not, is to **make a comparison.** Error bars can serve that goal by making your comparison of interest as salient as possible.

---

# General best practices

---

## General best practices

When you can, show raw data beneath the summary points and error bars.

```{r}
# TODO: Simulate dataset with 2 between-subjects conditions (european-american vs asian-american), one DV (emotional expression), one sensible covariate (age?)
# Bias the data in some kind of way such that the covariate partially explains the DV
# Add a few outliers

random_age <- function (race) {
  
  # .2 (10-20) + .4 (20-30) + .3 (30-60) + .1 (60-80)
  
  if (race == "euro_am") {
    prob_breakdowns <- c(.2, .4, .3/3, .1/2)
  } else if (race == "as_am") {
    prob_breakdowns <- c(.2, .55, .2/3, .05/2)
  }
  
  tens <- sample(seq(10, 70, 10),
                 size = 1,
                 replace = TRUE,
                 prob = rep(prob_breakdowns, times = c(1, 1, 3, 2)))
  
  ones <- sample(0:9,
                 size = 1,
                 replace = TRUE)
  
  return (as.integer(tens + ones))
}

# This counts id from 1:n separately for the two races, so that number is the n per group
d <- crossing(id = 1:60, race = c("euro_am", "as_am")) %>% 
  mutate(age = map_int(race, random_age),
         age_scaled = (age-25)/10,
         race = factor(race, levels = c("euro_am", "as_am")),
         intercept = 0,
         beta_race = -0.5,
         beta_age = -0.15,
         emo = intercept + beta_race*(race == "as_am") + beta_age*age_scaled + rt(n(), df = 5))
```

.pull-left[

```{r}

```

]

.pull-right[

```{r}

```

]

---

## General best practices

Use crossbars on the ends of error bars _thoughtfully,_ if at all.

TODO: Put graphs underneath

---

## General best practices

Where appropriate, choose error visualizations that highlight the density of the error distribution.

Error bars may lead people to view the data in a binary way. Specifically, error bars may lead people to treat the sampling distribution within the shown error bar as uniform and ignore the sampling distribution outside the error bar.

TODO: examples of histogram, violin, fat-skinny error bars

---

## General best practices

For better or worse, _people tend to use error bars as a visual marker of statistical significance._ Try to **accommodate** this tendency while remaining faithful to your comparison of interest.

--

Choose whether to show errors +- 1 SE, +- 1 CI, or something else to _highlight the size of your effect of interest._

--

For example, when comparing a test statistic against a 0, show the CI. Accommodate the heuristic that "if the error bar clears 0, the results are 'significant'."

TODO: Show example of when error bar maps on to the comparison of interest vs. when it does not

---

# Various scenarios

---

## Two-sample t-test

---

## OLS regression with one binary predictor variable

### Option 1: Compare difference to 0

Pro: 

- Analytical: This method allows you to use the SE provided by `lm()` without having to worry about finding the correct formula. If your regression is specified correctly, `lm()` will pool your SE for you!
- Numerical: By calculating a single sampling distribution of _differences,_ you can get a more accurate (and often slightly smaller) SE.

Con: While it's still on the scale of the data, this method obscures the values of the two group means.

---

## OLS regression with one binary predictor variable

### Option 2: Compare conditions to each other

[REVISE/REMOVE THIS TEXT?] When comparing two test statistics against each other, show the standard errors of each one. Accommodate the heuristic that "if the error bars clear each other, the results are 'significant'."

It's not a perfect heuristic, but showing longer CIs is likely to lead people to underestimate your effect size (not fair to you!).

---

## OLS multiple regression

In the simplest case, plot the raw data with a regression slope and error ribbon on top.

Even for variables where you have measurements at discrete points along a continuous variable, an error ribbon will help to remind people that there is also uncertainty between the measurement points.

(It will not accurately represent the true uncertainty of the interpolation, but it's better than nothing.)

.pull-left[

```{r}

```

]

--

.pull-right[

```{r}

```

]

---

Be aware that calling `geom_smooth(method = "lm")` on data split by a categorical variable will plot CIs for _simple effect regressions fit separately to subsets of data, **not** for the multiple regression interaction fit to all the data._

---

## Bootstrapping

In addition to the methods for plotting analytical sampling error, a new error visualization becomes available with a physical sampling distribution: a plot of the sampling distribution itself.

```{r}
d_b <- d %>%
  rsample::bootstraps(times = 500) %>%
  rename(iteration = id) %>% 
  # rsample automatically labels the iterations as "Bootstrap%03d"
  # want these just to be integers pls
  mutate(iteration = as.integer(str_sub(iteration, start = -3L)),
         # rsample stores the splits as special "splits" objects
         # which don't take up the same amount of memory as a fully resampled dataframe
         # so you need to call as.data.frame on them before you run the model
         coefs_boot = map(splits, ~.x %>%
                            as.data.frame() %>%
                            lm(emo ~ race * age_scaled, data = .) %>%
                            broom::tidy())) %>% 
  # drop the split objects, don't need the resampled data anymore
  select(-splits) %>% 
  # et voila, all your model parameters in a dataframe that's long by bootstrap iteration
  unnest(coefs_boot)
```

---

If you have the space for it, you can show a histogram:

```{r}

```

---

Or a density plot:

```{r}

```

---

In some situations, showing a violin plot of the sampling distribution can be an "upgraded" version of just showing a vertical error bar with a point estimate of a test statistic.

The `draw_quantiles` argument of `geom_violin()` allows easy plotting of particular percentiles of the sampling distribution.

```{r, out.height="100%", fig.asp = 5}

```

---

## Bayesian regression

We like using "spaghetti error bars" to show a sampling distribution of regression lines.

This method leverages transparency/opacity as an additional visual heuristic for the density of the distribution, and helps avoid the binary appearance of error ribbons.
