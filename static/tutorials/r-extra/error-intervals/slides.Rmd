---
title: "Presentation Ninja"
subtitle: "âš”<br/>with xaringan"
author: "Monica Thieu & Paul Bloom"
institute: "Dept of Psychology, Columbia University"
date: "2021-03-10 (updated: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
    css: ["theme.css", "fonts.css"]
    lib_dir: libs
    nature:
      highlightLanguage: r
      highlightStyle: solarized-light
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
require(tidyverse)
require(magrittr)
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(echo = FALSE, dev = "svglite")

se <- function (x, ...) {return (sd(x, ...)/sqrt(length(x)))}
```

# Outline

--

First, we'll review error intervals based on _how they're calculated._

--

Then, we'll explore different techniques for visualizing those intervals.

---

class: inverse, middle

# Two different types of error intervals

---

.pull-left[

## Analytical intervals

- calculated _exactly_ with **formulas**
- computationally _fast_
- _wholly_ reliant on distributional assumptions

]

.pull-right[

## Numerical intervals

- calculated _approximately_ over many **iterations**
- computationally _slower_
- _less_ reliant on distributional assumptions (and can work in the absence of assumptions!)

]

--

In the majority of cases, errors should agree when estimated using either method. In general, we recommend taking more time to estimate numerical intervals _unless:_

- You are confident that distributional assumptions will hold
- It is temporally/computationally infeasible to estimate numerical intervals

TODO: note regularly that we like methods that make you look at your data/intermediate outputs

---

class: inverse, middle

# Analytical error intervals

--

- Ordinary least squares intervals

--

- Maximum likelihood intervals

---

## OLS intervals

This encompasses the parameter estimates and standard errors calculated from _ordinary least squares regressions._

--

Remember, OLS regressions capture a [variety](https://lindeloev.github.io/tests-as-linear/) of statistical scenarios (t-tests, correlations, ANOVAs, and more).

--

In general, the error intervals we can get from these tests are a **standard error of the parameter estimate** and its associated **confidence interval.**

--

These values are directly related to one another by the following general formula:

$$CI_p = \overline{T} \pm t_{crit} \times SE$$

--

In most cases, the distance from the test statistic to one CI bound is equal to the standard error times the critical value for the desired confidence level.

--

If we assume a 95% confidence interval, and that the sampling distribution of the test statistic is _t_-distributed or normally distributed, the CI is approximately equal to the test statistic $\pm$ 2 SEs.

---

Don't forget! Dr. Niall Bolger sez:

--

**Over many theoretical repeated runs of a study, N% of the N% confidence intervals for all runs of the study are expected to overlap with the true parameter.**

--

A single calculation of the N% confidence interval does _not_ reflect an N% probability that the true parameter lies within that interval.

---

## ML intervals

TODO: Incorporate photo of parameter landscape to hammer home that when you don't know the landscape you can't take the second derivative

--

Maximum likelihood models can fit multilevel data, or other data that cannot readily be modeled with ordinary least squares regression.

--

ML models, like those from `lme4`, produce standard errors by taking the second derivative of the parameter log-likelihood function with respect to the parameter of interest, $\theta$, at the value of the maximum-likelihood estimate of said parameter, $\hat{\theta}$.

--

When the log-likelihood function is estimated well, ML standard errors are a good approximation of the standard deviation of the sampling distribution for multilevel model coefficients.

--

However, if ML estimation _fails to converge_ on a set of parameter estimates, this may mean that the log-likelihood function _violates the assumptions needed_ to consider the SE a valid estimate.

--

See [Penn State STAT 504 Spring 2005 lecture slides](https://personal.psu.edu/abs12/stat504/Lecture/lec3_4up.pdf) for further reference.

---

class: inverse, middle

# Numerical intervals

--

- Bootstrapping

--

- Bayesian Monte Carlo sampling

---

## Bootstrapping

Randomly resample complete observations from the base dataset with replacement many times, calculating the test statistic once for every resample. Collate all estimates of the test statistic across bootstrap iterations into a sampling distribution.

TODO: say it's important to LOOK at the bootstrapping distribution

If the resulting sampling distribution is normal, the bootstrapped standard error is equal to the **standard deviation of the sampling distribution.**

If the resulting sampling distribution is asymmetrical (and not normal), the lower and upper bounds of bootstrapped standard error interval can be calculated by taking the **16% and 84% percentiles of the sampling distribution.** (If the sampling distribution is symmetrical but not normal, this is equivalent to taking the middle 68% of the distribution.)

---

### Statistics you can bootstrap:

A sampling distribution can be bootstrapped for _basically any_ test statistic, no matter how complex its calculation is. Thus, bootstrapping is especially useful in cases where the test statistic might not have a normal sampling distribution, and analytical intervals might be inappropriate.

TODO: List things you can bootstrap

---

## Bayesian Monte Carlo sampling

---

class: inverse, middle

# Visualization

---

The purpose of any graph, error bars or not, is to **make a comparison.**

Try to _balance_ between **matching** visual features to your key results _and_ **accommodating** viewers' graph-reading heuristics.

--

Perhaps the biggest heuristic to consider is that _people tend to use error bars as a visual marker of statistical significance._ Concerns about significance aside, we can try to accommodate this.

--

When comparing a test statistic against a constant (often 0), show the CI. Accommodate the heuristic that "if the error bar clears 0, the results are 'significant'."

**This is a surprisingly powerful method for turning complicated test statistics into digestible visualizations.**

--

TODO: Show example of when error bar maps on to the comparison of interest vs. when it does not

---

# General best practices

---

When you can, show raw data beneath the summary points and error bars.

```{r}
# TODO: Simulate dataset with 2 between-subjects conditions (european-american vs asian-american), one DV (emotional expression), one sensible covariate (age?)
# Bias the data in some kind of way such that the covariate partially explains the DV
# Add a few outliers

sim <- crossing(obs = 1:20, x = 1:3) %>% 
  mutate(y = x + rnorm(n(), 0, 0.5))

sim_summary <- sim %>%
  group_by(x) %>% 
  summarize(mean_y = mean(y), sd_y = sd(y), se_y = se(y))
```

.pull-left[

```{r}
this_plot <- sim_summary %>% 
  ggplot(aes(x = x, y = mean_y)) +
  geom_line() +
  geom_point(aes(y = mean_y))

this_plot
```

]

.pull-right[

```{r}
sim_summary %>% 
  ggplot(aes(x = x, y = mean_y)) +
  geom_jitter(aes(y = y),
              data = sim,
              alpha = 0.2, width = 0.05, height = 0) +
  geom_line() +
  geom_point(aes(y = mean_y))
```

]

---

Use crossbars on the ends of error bars _thoughtfully,_ if at all.

TODO: Put graphs underneath

---

Where appropriate, choose error visualizations that highlight the density of the error distribution.

Error bars may lead people to view the data in a binary way. Specifically, error bars may lead people to treat the sampling distribution within the shown error bar as uniform and ignore the sampling distribution outside the error bar.

TODO: examples of histogram, violin, fat-skinny error bars

---

# Various scenarios

---

## Two-sample t-test

---

### Test statistic vs. 0: Analytical

```{r}
n_obs <- 50
effect_size <- 0.5
d <- tibble(y = rnorm(n_obs, effect_size, 1))

d_s <- d %>% 
  summarize(across(y, list(mean = mean, sd = sd, se = se))) %>% 
  mutate(y_rank = dense_rank(y_mean))

plot_base <- d_s %>% 
  ggplot(aes(x = 1, y = y_mean)) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_hline(yintercept = effect_size, color = "seagreen3", linetype = "dashed") +
  geom_errorbar(aes(ymin = y_mean - qt(.975, n_obs-1)*y_se,
                    ymax = y_mean + qt(.975, n_obs-1)*y_se),
                width = 0)
```

.pull-left[

```{r}
plot_base +
  geom_point()
```

]

.pull-right[

```{r}
plot_base +
  geom_errorbar(aes(ymin = y_mean - y_se,
                    ymax = y_mean + y_se),
                width = 0,
                size = 1) +
  geom_point()
```

]

---

### Test statistic vs. constant: Numerical

In addition to the methods for plotting analytical sampling error, a new error visualization becomes available with a physical sampling distribution: a plot of the sampling distribution itself.

```{r}
d_b <- d %>%
  rsample::bootstraps(times = 500) %>%
  mutate(boot_mean = map_dbl(splits, ~.x %>%
                               as.data.frame() %>%
                               pull(y) %>%
                               mean()))
```

---

If you have the space for it, you can show a histogram:

```{r}
d_b %>% 
  ggplot(aes(x = boot_mean)) +
  geom_histogram(bins = 30) +
  geom_vline(xintercept = 0, linetype = "dashed") +
  geom_vline(xintercept = effect_size, color = "seagreen3", linetype = "dashed")
```

---

Or a density plot:

```{r}
d_b %>% 
  ggplot(aes(x = boot_mean)) +
  geom_density(outline.type = "full") +
  geom_vline(xintercept = 0, linetype = "dashed") +
  geom_vline(xintercept = effect_size, color = "seagreen3", linetype = "dashed")
```

---

In some situations, showing a violin plot of the sampling distribution can be an "upgraded" version of just showing a vertical error bar with a point estimate of a test statistic.

The `draw_quantiles` argument of `geom_violin()` allows easy plotting of particular percentiles of the sampling distribution.

```{r, out.height="100%", fig.asp = 5}
d_b %>% 
  ggplot(aes(x = 1, y = boot_mean)) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_hline(yintercept = effect_size, color = "seagreen3", linetype = "dashed") +
  geom_violin(alpha = 0.5, draw_quantiles = c(.025, .975)) +
  geom_point(aes(y = y_mean),
             data = d_s)
```

---

## Comparing two test statistics to each other

### Option 1: Compare the difference to 0

Pro: 

- Analytical: This method allows you to use the SE provided by `lm()` without having to worry about finding the correct formula. If your regression is specified correctly, `lm()` will pool your SE for you!
- Numerical: By calculating a single sampling distribution of _differences,_ you can get a more accurate (and often slightly smaller) SE.

Con: While it's still on the scale of the data, this method obscures the values of the two test statistics.

Per the previous example, you can show these errors with a CI bar or sampling distribution.

---

## Comparing two test statistics to each other

### Option 2: Compare the values to each other

When comparing two test statistics against each other, show the standard errors of each one. Accommodate the heuristic that "if the error bars clear each other, the results are 'significant'."

It's not a perfect heuristic, but showing longer CIs is likely to lead people to underestimate your effect size (not fair to you!).

---

## Ordered data at discrete levels of x

For variables where you have measurements at discrete points along a continuous variable, an error ribbon will help to remind people that there is also uncertainty between the measurement points.

(It will not accurately represent the true uncertainty of the interpolation, but it's better than nothing.)

As with the prior methods, use standard errors when comparing two conditions against each other, and CIs when comparing a condition against 0 or another constant.

.pull-left[

```{r}
sim_summary %>% 
  ggplot(aes(x = x, y = mean_y)) +
  geom_jitter(aes(y = y),
              data = sim,
              alpha = 0.2, width = 0.05, height = 0) +
  geom_line() +
  geom_errorbar(aes(ymin = mean_y - se_y, ymax = mean_y + se_y), width = 0)
  geom_point(aes(y = mean_y))
```

]

--

.pull-right[

```{r}
sim_summary %>% 
  ggplot(aes(x = x, y = mean_y)) +
  geom_jitter(aes(y = y),
              data = sim,
              alpha = 0.2, width = 0.05, height = 0) +
  geom_line() +
  geom_ribbon(aes(ymin = mean_y - se_y, ymax = mean_y + se_y), alpha = 0.2) +
  geom_point(aes(y = mean_y))
```

]

---

## Regression slope

In the simplest case, plot the raw data with a regression slope and error ribbon on top.

### Regression slope: Analytical

Be aware that calling `geom_smooth(method = "lm")` on data split by a categorical variable will plot CIs for _simple effect regressions fit separately to subsets of data, **not** for the multiple regression interaction fit to all the data._ 

### Regression slope: Numerical

We like using "spaghetti error bars" to show a sampling distribution of regression lines.

This method leverages transparency/opacity as an additional visual heuristic for the density of the distribution, and helps avoid the binary appearance of error ribbons.
