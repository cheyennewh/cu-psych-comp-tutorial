<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Presentation Ninja</title>
    <meta charset="utf-8" />
    <meta name="author" content="Monica Thieu &amp; Paul Bloom" />
    <link rel="stylesheet" href="theme.css" type="text/css" />
    <link rel="stylesheet" href="fonts.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Presentation Ninja
## âš”<br/>with xaringan
### Monica Thieu &amp; Paul Bloom
### Dept of Psychology, Columbia University
### 2021-03-10 (updated: 2021-03-10)

---




# Outline

--

First, we'll review error intervals based on _how they're calculated._

--

Then, we'll explore different techniques for visualizing those intervals.

---

class: inverse, middle

# Two different types of error intervals

---

.pull-left[


## Analytical intervals

- calculated _exactly_ with **formulas**
- computationally _fast_
- _wholly_ reliant on distributional assumptions

]

.pull-right[

## Numerical intervals

- calculated _approximately_ over many **iterations**
- computationally _slower_
- _less_ reliant on distributional assumptions (and can work in the absence of assumptions!)

]

--

In the majority of cases, errors should agree when estimated using either method. In general, we recommend taking more time to estimate numerical intervals _unless:_

- You are confident that distributional assumptions will hold
- It is temporally/computationally infeasible to estimate numerical intervals

**Note:** one advantage to using numerical methods is that they will cause you to more carefully consider assumptions you can choose to make about your data &amp; intermediate outputs

---

class: inverse, middle

## How can we be confident in our intervals? 

![yo_dawg_meme](https://memegenerator.net/img/instances/64796890.jpg)

---

# Analytical error intervals

--

- Ordinary least squares intervals

--

- Maximum likelihood intervals

---

## OLS intervals

This encompasses the parameter estimates and standard errors calculated from _ordinary least squares regressions._

--

Remember, OLS regressions capture a [variety](https://lindeloev.github.io/tests-as-linear/) of statistical scenarios (t-tests, correlations, ANOVAs, and more).

--

In general, the error intervals we can get from these tests are a **standard error of the parameter estimate** and its associated **confidence interval.**

--

These values are directly related to one another by the following general formula:

`$$CI_p = \overline{T} \pm t_{crit} \times SE$$`

--

In most cases, the distance from the test statistic to one CI bound is equal to the standard error times the critical value for the desired confidence level.

--

If we assume a 95% confidence interval, and that the sampling distribution of the test statistic is _t_-distributed or normally distributed, the CI is approximately equal to the test statistic `\(\pm\)` 2 SEs.

---

Don't forget! Dr. Niall Bolger sez:

--

**Over many theoretical repeated runs of a study, N% of the N% confidence intervals for all runs of the study are expected to overlap with the true parameter.**

--

A single calculation of the N% confidence interval does _not_ reflect an N% probability that the true parameter lies within that interval.

--

So, _you can make a confidence interval, but you can't be confident in your interval_ - Dr. Niall Bolger


---

## ML intervals



Maximum likelihood models can fit multilevel data, or other data that cannot readily be modeled with ordinary least squares regression.

--

ML models, like those from `lme4`, produce standard errors by taking the second derivative of the parameter log-likelihood function with respect to the parameter of interest, `\(\theta\)`, at the value of the maximum-likelihood estimate of said parameter, `\(\hat{\theta}\)`.

--

When the log-likelihood function is estimated well, ML standard errors are a good approximation of the standard deviation of the sampling distribution for multilevel model coefficients.

--

However, if ML estimation _fails to converge_ on a set of parameter estimates, this may mean that the log-likelihood function _violates the assumptions needed_ to consider the SE a valid estimate.

--

See [Penn State STAT 504 Spring 2005 lecture slides](https://personal.psu.edu/abs12/stat504/Lecture/lec3_4up.pdf) for further reference.

---

## Drawbacks with Analytical Intervals: The "Mountain" Problem


OLS &amp; ML estimation assume that paramters are normally distributed

--


Imagine estimating the contour of a mountain using two different strategies
  * Hiking up to the peak of a mountain &amp; getting one measurement of the steepness of the slope on your way up
  * Putting on a blindfold and randomly walking around the mountain for days, repeatedly recording your height above sea level
  
--

&lt;img src="https://upload.wikimedia.org/wikipedia/commons/thumb/5/57/Multivariate_Gaussian.png/1024px-Multivariate_Gaussian.png" alt="drawing" width="500" height = "275"/&gt;

---

## What if your mountain looks like this?

![non normal mountain 1](https://media.springernature.com/original/springer-static/image/chp%3A10.1007%2F978-3-319-27284-9_9/MediaObjects/393463_1_En_9_Fig1_HTML.gif)

---

## Or this?

![non normal mountain 1](https://miro.medium.com/max/1454/1*s1Uk0_3NY1DXrRWcb17QMQ.png)


---

class: inverse, middle

# Numerical intervals

--

- Bootstrapping

--

- Bayesian Monte Carlo sampling

---

## Bootstrapping

Randomly resample complete observations from the base dataset with replacement many times, calculating the test statistic once for every resample. Collate all estimates of the test statistic across bootstrap iterations into a sampling distribution.


![boostrap](https://yashuseth.files.wordpress.com/2017/12/bootstrap.png)


---

### Statistics you can bootstrap:

* You can bootstrap _basically any_ test statistic, no matter how complex
* Especially useful where the test statistic might not have a normal sampling distribution, and analytical intervals might be inappropriate.

  * Mean, median, standard deviation of a distribution
  * Regression coefficients
  * Prediction performance (accuracy, F1, AUC, R2)
  * Unsupervised learning (PCA, clustering)
  * Reliability (ICC) or intra-item consistency
  * Tree-based models (i.e. 'bagging' in a random forrest)
  
&lt;img src="https://advstats.psychstat.org/book/images/bootstrap.svg" alt="drawing" width="800" height = "250"/&gt;

---

## Bayesian Monte Carlo sampling

"Hiker + blindfold" 

&lt;img src="https://mc-stan.org/images/feature/wide_ensemble.png" alt="drawing" width="800" height = "250"/&gt;

--

* Similar to bootstrapping, can estimate _any test statistic_ with _any distribution_
* Can also incorporate _priors_
* With R or python, we commonly implement MCMC sampling using [Stan](https://mc-stan.org/) on the back end
  * Packages [brms](https://cran.r-project.org/web/packages/brms/index.html) and [rstanarm](https://cran.r-project.org/web/packages/rstanarm/index.html) allow us to do this for regressions using the same R syntax as `lm()` and `lmer()`

---

## Why use Bayesian Monte Carlo methods?

Analytical approaches _will_ fail you sooner or later
```
Warning messages:
1: In checkConv(attr(opt, "derivs"), optpar,ctrl=controlpar,ctrl=controlcheckConv, 
: unable to evaluate scaled gradient
2: In checkConv(attr(opt, "derivs"), optpar,ctrl=controlpar,ctrl=controlcheckConv, 
: Model failed to converge: degenerate Hessian with 1 negative eigenvalues
```

--

* Bayesian inference with weakly informative priors can keep estimates reasonable
* Especially for models with _many_ parameters like multilevel models, Bayesian priors + MCMC provide the constraints necessary
* telling the hiker "On the side of the mountain there most likely there isn't a 1-foot diameter pit that goes all the way to the center of the earth"
  
  
---


## Bayesian intervals: can you be confident in them?

* Yes! Bayesian models estimate `\(P(parameters|data)\)`
* So, intervals based on the model's posterior distribution can be thought of as intervals where "the model is N% sure that the true value of the parameter is"
* For this reason, hip &amp; alternative Bayesians usually use phrases that aren't 'confidence intervals', like _Credible Intervals_ , _Posterior Intervals_, _Uncertainty Intervals_, or _Highest Posterior Density Interval (HPDI)_



---


class: inverse, middle

# Visualization

---

class: center, middle

The purpose of any graph, error bars or not, is to **make a comparison.** Error bars can serve that goal by making your comparison of interest as salient as possible.

---

# General best practices

---

## General best practices

When you can, show raw data beneath the summary points and error bars.




.pull-left[

![](slides_files/figure-html/unnamed-chunk-2-1.svg)&lt;!-- --&gt;

]

.pull-right[

![](slides_files/figure-html/unnamed-chunk-3-1.svg)&lt;!-- --&gt;

]

---

## General best practices

Use crossbars on the ends of error bars _thoughtfully,_ if at all.

--

Choose whether to show errors +- 1 SE, +-2 SE, 80% CI, 90%, 95% CI, 99% CI, or something else to _highlight the size of your effect of interest._

---

## General best practices

Where appropriate, choose error visualizations that highlight the density of the error distribution.






![](slides_files/figure-html/unnamed-chunk-4-1.svg)&lt;!-- --&gt;


---

Error bars may lead people to view the data in a binary way. Specifically, error bars may lead people to treat the sampling distribution within the shown error bar as uniform and ignore the sampling distribution outside the error bar.

Error bars below show 80% and 99% intervals. 

--

.pull-left[

![](slides_files/figure-html/unnamed-chunk-5-1.svg)&lt;!-- --&gt;

]


.pull-right[

![](slides_files/figure-html/unnamed-chunk-6-1.svg)&lt;!-- --&gt;
]

---

## General best practices

For better or worse, _people tend to use error bars as a visual marker of statistical significance._ Try to **accommodate** this tendency while remaining faithful to your comparison of interest.

--

For example, when comparing a test statistic against a 0, show the CI. Accommodate the heuristic that "if the error bar clears 0, the results are 'significant'."

.pull-left[

![](slides_files/figure-html/unnamed-chunk-7-1.svg)&lt;!-- --&gt;

]




.pull-right[

![](slides_files/figure-html/unnamed-chunk-9-1.svg)&lt;!-- --&gt;

]

---
 
## Example: comparing two groups

* Here, we want to know whether emotional expression differs in our sample between European-American and Asian-American participant
* We'll use *regression* and generate uncertainty intervals for our parameters of interest


---



## Option 1: OLS regression with one binary predictor variable

* This method allows you to use the SE provided by `lm()` without having to worry about finding the correct formula. 
* It's quick, and you might already be familiar with `lm()` mdoel syntax
* Estimation might not be accurate if OLS assumptions aren't met
* We don't get as much detailed info on the uncertainty
* We get a *confidence* interval, so we can't be confident in it


---

## Option 1: OLS regression with one binary predictor variable

--
Run model

```
m_ols = lm(data = d, emo ~ race)
```
--


Get confidence intervals for the difference between groups
```
ols_interval_80 = confint(m_ols, 'raceas_am', level=0.80)
ols_interval_99 = confint(m_ols, 'raceas_am', level=0.99)
```

--

Get predictions for the mean of each group
```
prediction_grid= expand.grid(race = c('euro_am', 'as_am'))
prediction_frame_ols = predict(m_ols, newdata = prediction_grid, interval = 'confidence') %&gt;%
  data.frame() %&gt;% cbind(., prediction_grid)
```  
  
---





![](slides_files/figure-html/unnamed-chunk-11-1.svg)&lt;!-- --&gt;

---

## Option 2: Bootstrapping

Once we use numeric methods we have an additional bonus: access to the full sampling distribution for all parameters of interest

```
d_b &lt;- d %&gt;%
  rsample::bootstraps(times = 500) %&gt;% rename(iteration = id) %&gt;% 
  # rsample automatically labels the iterations as "Bootstrap%03d"
  # want these just to be integers pls
  mutate(iteration = as.integer(str_sub(iteration, start = -3L)),
         # rsample stores the splits as special "splits" objects
         # which don't take up the same amount of memory as a fully resampled dataframe
         # so you need to call as.data.frame on them before you run the model
         coefs_boot = map(splits, ~.x %&gt;% as.data.frame() %&gt;%
                            lm(emo ~ race * age_scaled, data = .) %&gt;%
                            broom::tidy())) %&gt;% 
  # drop the split objects, don't need the resampled data anymore
  select(-splits) %&gt;% 
  # et voila, all your model parameters in a dataframe that's long by bootstrap iteration
  unnest(coefs_boot)
```

---




We really care about the `estimate` where `term == raceas_am`  here, but we could bootstrap any quantity we want


```
## # A tibble: 6 x 6
##   iteration term        estimate std.error statistic p.value
##       &lt;int&gt; &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;
## 1         1 (Intercept)  -0.264      0.144   -1.84    0.0681
## 2         1 raceas_am    -0.196      0.189   -1.04    0.302 
## 3         2 (Intercept)  -0.341      0.187   -1.83    0.0704
## 4         2 raceas_am    -0.0246     0.279   -0.0884  0.930 
## 5         3 (Intercept)  -0.129      0.161   -0.804   0.423 
## 6         3 raceas_am    -0.0458     0.233   -0.196   0.845
```


---

This full distribution of boostrapped estimates for the `raceas_am` term is really what we want

```
d_b %&gt;%
  dplyr::filter(term == "raceas_am") %&gt;%
  ggplot(data = ., aes(x = estimate)) +
  geom_histogram(bins = 50) +
  geom_vline(xintercept = 0) +
  labs(x = 'Bootstrapped estimates of the mean differerence in emotional expression\nAsian-American &gt; European American', y = 'count') +
  theme_bw()
```


![](slides_files/figure-html/unnamed-chunk-14-1.svg)&lt;!-- --&gt;

---

Summarizing the bootstrap distribution


```
d_b_summary = d_b %&gt;%
  dplyr::filter(term == "raceas_am") %&gt;%
  summarise(mean_est = mean(estimate),
            lwr_80 = quantile(estimate, probs = .1),
            upr_80 = quantile(estimate, probs = .9),
            lwr_99 = quantile(estimate, probs = .005),
            upr_99 = quantile(estimate, probs = .995)) 
```

---

![](slides_files/figure-html/unnamed-chunk-16-1.svg)&lt;!-- --&gt;

---

## Option 3: Bayesian regression

Run the Bayesian regression in `rstanarm` -- equivalent syntax to `lm()`
```
m_bayes = rstanarm::stan_glm(data = d, emo ~ race)
```
--

Extract posterior draws to a dataframe
```
m_bayes_draws = as.data.frame(m_bayes)
```

--

Inspect posterior draws

```
##    (Intercept)   raceas_am    sigma
## 1  0.011525732 -0.47611875 1.352799
## 2 -0.238451302 -0.18768410 1.437119
## 3 -0.009920767 -0.18225474 1.468166
## 4 -0.124548952 -0.27101821 1.433276
## 5 -0.252410187  0.06184317 1.484872
## 6 -0.073118299 -0.28351642 1.417531
```
---

Summarize posterior distribution

```
m_bayes_draws_summary = m_bayes_draws %&gt;%
  summarise(mean_est = mean(raceas_am),
            lwr_80 = quantile(raceas_am, probs = .1),
            upr_80 = quantile(raceas_am, probs = .9),
            lwr_99 = quantile(raceas_am, probs = .005),
            upr_99 = quantile(raceas_am, probs = .995)) 
```
---

## Comparing analytical, bootstrap, and bayesian intervals

* In many settings, these methods will give similar results
* If there are discrepancies, _we almost always trust the numerical methods more_ 

![](slides_files/figure-html/unnamed-chunk-18-1.svg)&lt;!-- --&gt;
---

Only numerical methods will allow you to inspect the whole sampling distribution in detail though

![](slides_files/figure-html/unnamed-chunk-19-1.svg)&lt;!-- --&gt;


---

## What about a continuous x variable?

Let's run another Bayesian model looking at emotional expression as a function of age



```
mod_cont = rstanarm::stan_glm(emo ~ age, data = d)
```
--

Then we can get draws of the linear predictor with respect to age

```
age_grid = expand.grid(age = min(d$age):max(d$age))
age_predictions = tidybayes::linpred_draws(model = mod_cont, newdata = age_grid)
```

---


```
## # A tibble: 6 x 4
## # Groups:   age, .row [1]
##     age  .row .draw  .value
##   &lt;int&gt; &lt;int&gt; &lt;int&gt;   &lt;dbl&gt;
## 1    10     1     1  0.415 
## 2    10     1     2  0.217 
## 3    10     1     3  0.0821
## 4    10     1     4  0.616 
## 5    10     1     5 -0.0218
## 6    10     1     6  0.116
```

---




.pull-left[

Spaghetti! 
![](slides_files/figure-html/unnamed-chunk-23-1.svg)&lt;!-- --&gt;
]


.pull-right[

Ribbons! 
![](slides_files/figure-html/unnamed-chunk-24-1.svg)&lt;!-- --&gt;

]

---

.pull-left[

Spaghetti + raw data! 
![](slides_files/figure-html/unnamed-chunk-25-1.svg)&lt;!-- --&gt;
]


.pull-right[

Ribbons + raw data! 
![](slides_files/figure-html/unnamed-chunk-26-1.svg)&lt;!-- --&gt;

]


---

## Discussion Points

* We *always* want to make sure we quantify uncertainty for the key comparison of interest
* It is also good to plot and quantify uncertainty from multiple angles (i.e. group means vs. difference)
* Visualizing your raw data helps you understand your uncertainty
* Numeric methods are almost always a good idea if feasible

---


## Code Implementation

We did *not* focus on the syntax behind working with these uncertainty intervals as much, but you can find all code for this presentation on the Columbia Psych Computing [Website](https://cu-psych-computing.github.io/cu-psych-comp-tutorial/tutorials/r-extra/) &amp; [Github](https://github.com/cu-psych-computing/cu-psych-comp-tutorial/tree/master/static/tutorials/r-extra)
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightLanguage": "r",
"highlightStyle": "solarized-light",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
