---
title: "R Tutorial: Interpreting Regression Models"
output: 
  html_document: 
    toc: true
    toc_float: true
--- 
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Interpreting Regression Models

Although we've presented several types of models here, all of them are, at their root, cases of regression! Although regression coefficients are sometimes difficult to interpret, we think this is crucial! Below, we go through several examples:

*Note, we'll also load the arm package, which helps to display model outputs in a handy way*

### Regression with a categorical (binary) predictor

Let's estimate dprime as a function of adhdF group ('High ADHD' or 'Low ADHD') here
```{r}
suppressWarnings(suppressMessages(library(arm)))
categoricalRegMod <- lm(dprime ~ adhdF, data = df)
display(categoricalRegMod)
```

Okay, so let's break down what this means.

  * First, we see our model equation at the top of the output. Handy!
  * **(Intercept)**: On the next line, we have estimates for the intercept of this model, and an estimate of the standard error for this intercept. Crucially, the intercept here in the regression model is the *estimated value of the outcome variable when all predictors are at their default.* Ah, but what's this 'default'? This is something we have to be very careful with when setting up our models.
    * For numerical predictors the default is 0
    * For character variables (strings) the default is the first alphabetical possibility
    * For factor variables the default is the first level of the factor (level 1). 
    * So here, average dprime is estimated to be 1.13 with standard error 0.09 for 'High ADHD' subjects
    * Note that the intercept here is exactly the same as the estimate for the 'High ADHD' group in the two-sample t-test earlier
  * **adhdFlow ADHD**: On the third line, we have the estimate and estimated standard error for the beta coefficient for adhdF -- our binary variable that takes values 'Low ADHD' and 'High ADHD'. We can interpret the estimate such that on average, an individual in the 'Low ADHD' group is estimated to have a dprime score -0.11 units different (with a standard error of .13) units from an individual in the 'High ADHD' group. 
    * Importantly, it is pretty clear that we don't have a very accurate estimate for our adhdF predictor here! If the standard error is 0.13 dprime units when the estimated value of the predictors is only -0.11 dprime units, our uncertainty in the estimated average difference between the two groups is huge compared to the estimated difference itself.


### Regression with a continuous predictor 

Let's estimate dprime as a function of a continuous variable, the number of hits (hitCount). This variable ranges from 1-20

```{r}
continuousRegMod = lm(dprime ~ hitCount, data = df)
display(continuousRegMod)
```

Okay, let's interpret.

  * **Intercept**: The intercept is tricky here. This estimate is for the average subject who has a value of 0 for hitCount. This isn't possible!
    * This is definitely something to watch out for! One solution to this problem (so our intercept will be more meaningful) is to mean-center our continuous predictor, so that the intercept will represent the estimated dprime for a subject with an *average* hitCount score.
  * **hitCount**: The hitCount predictor here indicates that on average, an individual with 1 more hit than another individual is estimated to have a dprime score 0.07 higher, with a standard error of .01. 
    * Compared to the estimate of 0.07, the standard error of the hitCount estimate 0.01 is pretty small, so we have relatively low uncertainty in our estimate of the hitCount predictor. (This makes a lot of sense, because the hitCount is probably being used to calculate dprime here)

### Regression with a mean-centered continuous predictor

Mean-centering is often a good idea for interpreting our models. To make the intercept easier to interpret, let's try mean-centering hitCount here.

```{r}
# check what the mean for hitCount is and mean-center hitCount
mean(df$hitCount)
df = mutate(df,
            hitCountMeanCenter = hitCount - mean(hitCount))


continuousRegMeanCenterMod = lm(dprime ~ hitCountMeanCenter, data = df)
display(continuousRegMeanCenterMod)
```

While mean-centering might feel confusing at first, this model is actually much easier to interpret in many ways.

Importantly:

  * All the estimates for the hitCount predictor are unchanged by mean centering! In a linear regression with one predictor, this will always be true (but we have to be careful about this if there are interaction terms)
  * The intercept now represents the estimated dprime for an average subject at the average hitCount, `r mean(df$hitCount)`. This is much more helpful than the estimated dprime for a subject with 0 hits! Notice that the standard error for this estimate has also gone down from the previous model -- this is because it is easier to accurately estimate an intercept at the mean level of a predictor than at a value for which no data exist!
